# AdversarialAttackDetection
This repository contains the demo for the paper "Reconstruction-based Adversarial Attack Detection on Deep Learning Model in Vision-based Autonomous Driving Systems."

**Demo Link for Gaussian noise Perturbation-based Advesarial Attack**

[Gaussian Nosie Perturbation-based Adversarial Attack](https://youtu.be/jUgBWJme5pA)

**Demo Link for Speckle noise Perturbation-based Advesarial Attack**

[Demo Video for Speckle Noise Perturbation-based Adversarial Attack](https://youtu.be/rtn8LrLXbDE)

**Demo Link for Shot noise Perturbation-based Advesarial Attack**

[Demo Video for Shot Noise Perturbation-based Adversarial Attack](https://youtu.be/URSU_BHP1ks)

**Demo Link for Impulse noise Perturbation-based Advesarial Attack**

[Demo Video for Impulse Noise Perturbation-based Adversarial Attack](https://youtu.be/9j_Le3tBcgk)



**Caption**
The lift pane in the uppermost window shows the adversarially perturbed image frames, while the right window shows the normal image frame. The bottom window shows the impact of these attacks on autonomous driving systems.

# Description of Experiment
In this experiment, we used the modified NVIDIA-2 model for self-driving cars. It was trained using imitation learning. After training the model, we applied adversarial attacks to verify its robustness against adversarial attacks. Our detailed experimental results showed that the start-of-the-art models are also vulnerable to adversarial attacks. We also proposed a deep autoencoder-based adversarial attack detection that effectively detects any input image-specific adversarial attacks. The detailed experimental results can be found in the manuscript. 
This experiment is a part of the ongoing research in which we are making an adversarial testing tool. The tool soon be published on our GitHub repository. 
